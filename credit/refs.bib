@article{chgl,
  author={L. {Jenkins} and T. {Bhuiyan} and S. {Harun} and C. {Lightsey} and D. {Mentgen} and S. {Aksoy} and T. {Stavcnger} and M. {Zalewski} and H. {Medal} and C. {Joslyn}},
  booktitle={2018 IEEE High Performance extreme Computing Conference (HPEC)}, 
  title={Chapel HyperGraph Library (CHGL)}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={We present the Chapel Hpergraph Library (CHGL), a library for hypergraph computation in the emerging Chapel language. Hypergraphs generalize graphs, where a hypergraph edge can connect any number of vertices. Thus, hypergraphs capture high-order, high-dimensional interactions between multiple entities that are not directly expressible in graphs. CHGL is designed to provide HPC-class computation with high-level abstractions and modern language support for parallel computing on shared memory and distributed memory systems. In this paper we describe the design of CHGL, including first principles, data structures, and algorithms, and we present preliminary performance results based on a graph generation use case. We also discuss ongoing work of codesign with Chapel, which is currently centered on improving performance.},
  keywords={data structures;distributed memory systems;graph theory;parallel processing;graph generation;distributed memory systems;shared memory;parallel computing;modern language support;high-level abstractions;HPC-class computation;multiple entities;high-dimensional interactions;hypergraphs capture high-order;hypergraph edge;emerging Chapel language;hypergraph computation;CHGL;Chapel HyperGraph Library;Arrays;Libraries;Reactive power;Data models;Task analysis;Programming},
  doi={10.1109/HPEC.2018.8547520},
  ISSN={2377-6943},
  month={Sep.},}

@article{arkouda,
author = {Merrill, Michael and Reus, William and Neumann, Timothy},
title = {Arkouda: Interactive Data Exploration Backed by Chapel},
year = {2019},
isbn = {9781450368001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329722.3330148},
doi = {10.1145/3329722.3330148},
abstract = {Exploratory data analysis (EDA) is the prerequisite for all data science. EDA is non-negotiably interactive—by far the most popular environment for EDA is a Jupyter notebook—and, as datasets grow, increasingly computationally intensive. Several existing projects attempt to combine interactivity and distributed computation using programming paradigms and tools from cloud computing, but none of these projects have come close to meeting our needs for high-performance EDA. To fill this gap, we have developed a prototype, called arkouda, that allows a user to interactively issue massively parallel computations on distributed data.},
booktitle = {Proceedings of the ACM SIGPLAN 6th on Chapel Implementers and Users Workshop},
pages = {28},
numpages = {1},
keywords = {Chapel, Python, NumPy, Jupyter, Exploratory data analysis (EDA)},
location = {Phoenix, AZ, USA},
series = {CHIUW 2019}
}

@article{bcl,
      title={BCL: A Cross-Platform Distributed Container Library}, 
      author={Benjamin Brock and Aydın Buluç and Katherine Yelick},
      year={2019},
      eprint={1810.13029},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@book{fouss_saerens_shimbo_2016, place={Cambridge}, title={Algorithms and Models for Network Data and Link Analysis}, DOI={10.1017/CBO9781316418321}, publisher={Cambridge University Press}, author={Fouss, François and Saerens, Marco and Shimbo, Masashi}, year={2016}}

@article{upcpp,
author={Y. {Zheng} and A. {Kamil} and M. B. {Driscoll} and H. {Shan} and K. {Yelick}},
booktitle={2014 IEEE 28th International Parallel and Distributed Processing Symposium}, 
title={UPC++: A PGAS Extension for C++}, 
year={2014},
volume={},
number={},
pages={1105-1114},
doi={10.1109/IPDPS.2014.115}}

@article{azadb17a,
  author    = {Ariful Azad and
               Aydin Bulu{\c{c}}},
  title     = {Towards a GraphBLAS Library in Chapel},
  booktitle = {2017 {IEEE} International Parallel and Distributed Processing Symposium
               Workshops, {IPDPS} Workshops 2017, Orlando / Buena Vista, FL, USA,
               May 29 - June 2, 2017},
  pages     = {1095--1104},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/IPDPSW.2017.118},
  doi       = {10.1109/IPDPSW.2017.118},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/ipps/AzadB17a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{graphlinalg,
author = {Kepner, Jeremy and Gilbert, John},editor = {Jeremy Kepner and John Gilbert},
title = {Graph Algorithms in the Language of Linear Algebra},
publisher = {Society for Industrial and Applied Mathematics},
year = {2011},
doi = {10.1137/1.9780898719918},
address = {},
edition   = {},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9780898719918},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898719918}
}

@INPROCEEDINGS{graphblasmath,
  author={J. {Kepner} and P. {Aaltonen} and D. {Bader} and A. {Buluç} and F. {Franchetti} and J. {Gilbert} and D. {Hutchison} and M. {Kumar} and A. {Lumsdaine} and H. {Meyerhenke} and S. {McMillan} and C. {Yang} and J. D. {Owens} and M. {Zalewski} and T. {Mattson} and J. {Moreira}},
  booktitle={2016 IEEE High Performance Extreme Computing Conference (HPEC)}, 
  title={Mathematical foundations of the GraphBLAS}, 
  year={2016},
  volume={},
  number={},
  pages={1-9},
  doi={10.1109/HPEC.2016.7761646}}

@Inbook{blas,
author="van de Geijn, Robert
and Goto, Kazushige",
editor="Padua, David",
title="BLAS (Basic Linear Algebra Subprograms)",
bookTitle="Encyclopedia of Parallel Computing",
year="2011",
publisher="Springer US",
address="Boston, MA",
pages="157--164",
isbn="978-0-387-09766-4",
doi="10.1007/978-0-387-09766-4_84",
url="https://doi.org/10.1007/978-0-387-09766-4_84"
}

@article{pgaslangs,
editor="Padua, David",
title="Partitioned Global Address Space (PGAS) Languages",
bookTitle="Encyclopedia of Parallel Computing",
year="2011",
publisher="Springer US",
address="Boston, MA",
pages="1465--1465",
isbn="978-0-387-09766-4",
doi="10.1007/978-0-387-09766-4_2091",
url="https://doi.org/10.1007/978-0-387-09766-4_2091"
}

@book{patterson_hennessy,
author = {Patterson, David A. and Hennessy, John L.},
title = {Computer Organization and Design, Fifth Edition: The Hardware/Software Interface},
year = {2013},
isbn = {0124077269},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {5th},
abstract = {The 5th edition of Computer Organization and Design moves forward into the post-PC era with new examples, exercises, and material highlighting the emergence of mobile computing and the cloud. This generational change is emphasized and explored with updated content featuring tablet computers, cloud infrastructure, and the ARM (mobile computing devices) and x86 (cloud computing) architectures. Because an understanding of modern hardware is essential to achieving good performance and energy efficiency, this edition adds a new concrete example, "Going Faster," used throughout the text to demonstrate extremely effective optimization techniques. Also new to this edition is discussion of the "Eight Great Ideas" of computer architecture. As with previous editions, a MIPS processor is the core used to present the fundamentals of hardware technologies, assembly language, computer arithmetic, pipelining, memory hierarchies and I/O. Instructors looking for4th Edition teaching materials should e-mail textbook@elsevier.com. Includes new examples, exercises, and material highlighting the emergence of mobile computing and the Cloud. Covers parallelism in depth with examples and content highlighting parallel hardware and software topics Features the Intel Core i7, ARM Cortex-A8 and NVIDIA Fermi GPU as real-world examples throughout the book Adds a new concrete example, "Going Faster," to demonstrate how understanding hardware can inspire software optimizations that improve performance by 200 times. Discusses and highlights the "Eight Great Ideas" of computer architecture: Performance via Parallelism; Performance via Pipelining; Performance via Prediction; Design for Moore's Law; Hierarchy of Memories; Abstraction to Simplify Design; Make the Common Case Fast; and Dependability via Redundancy. Includes a full set of updated and improved exercises.}
}

@article{mapreduce,
title = "MapReduce: Simplified Data Analysis of Big Data",
journal = "Procedia Computer Science",
volume = "57",
pages = "563 - 571",
year = "2015",
note = "3rd International Conference on Recent Trends in Computing 2015 (ICRTC-2015)",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.07.392",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915019213",
author = "Seema Maitrey and C.K. Jha",
keywords = "Big Data, Data Mining, parallelization Techniques, HDFS, MapReduce, Hadoop.",
abstract = "With the development of computer technology, there is a tremendous increase in the growth of data. Scientists are overwhelmed with this increasing amount of data processing needs which is getting arisen from every science field. A big problem has been encountered in various fields for making the full use of these large scale data which support decision making. Data mining is the technique that can discovers new patterns from large data sets. For many years it has been studied in all kinds of application area and thus many data mining methods have been developed and applied to practice. But there was a tremendous increase in the amount of data, their computation and analyses in recent years. In such situation most classical data mining methods became out of reach in practice to handle such big data. Efficient parallel/concurrent algorithms and implementation techniques are the key to meeting the scalability and performance requirements entailed in such large scale data mining analyses. Number of parallel algorithms has been implemented by making the use of different parallelization techniques which can be listed as: threads, MPI, MapReduce, and mash-up or workflow technologies that yields different performance and usability characteristics. MPI model is found to be efficient in computing the rigorous problems, especially in simulation. But it is not easy to be used in real. MapReduce is developed from the data analysis model of the information retrieval field and is a cloud technology. Till now, several MapReduce architectures has been developed for handling the big data. The most famous is the Google. The other one having such features is Hadoop which is the most popular open source MapReduce software adopted by many huge IT companies, such as Yahoo, Facebook, eBay and so on. In this paper, we focus specifically on Hadoop and its implementation of MapReduce for analytical processing."
}
	
@INPROCEEDINGS{hadoop,
author={S. G. {Manikandan} and S. {Ravi}},
booktitle={2014 International Conference on IT Convergence and Security (ICITCS)}, 
title={Big Data Analysis Using Apache Hadoop}, 
year={2014},
volume={},
number={},
pages={1-4},
abstract={We live in on-demand, on-command Digital universe with data prolifering by Institutions, Individuals and Machines at a very high rate. This data is categories as "Big Data" due to its sheer Volume, Variety and Velocity. Most of this data is unstructured, quasi structured or semi structured and it is heterogeneous in nature. The volume and the heterogeneity of data with the speed it is generated, makes it difficult for the present computing infrastructure to manage Big Data. Traditional data management, warehousing and analysis systems fall short of tools to analyze this data. Due to its specific nature of Big Data, it is stored in distributed file system architectures. Hadoop and HDFS by Apache is widely used for storing and managing Big Data. Analyzing Big Data is a challenging task as it involves large distributed file systems which should be fault tolerant, flexible and scalable. Map Reduce is widely been used for the efficient analysis of Big Data. Traditional DBMS techniques like Joins and Indexing and other techniques like graph search is used for classification and clustering of Big Data. These techniques are being adopted to be used in Map Reduce. In this paper we suggest various methods for catering to the problems in hand through Map Reduce framework over Hadoop Distributed File System (HDFS). Map Reduce is a Minimization technique which makes use of file indexing with mapping, sorting, shuffling and finally reducing. Map Reduce techniques have been studied in this paper which is implemented for Big Data analysis using HDFS.},
keywords={Big Data;distributed databases;network operating systems;pattern clustering;software fault tolerance;big data analysis;Apache Hadoop;data heterogeneity;data volume;computing infrastructure;data management;data warehousing;data analysis systems;HDFS;fault tolerant;DBMS techniques;joins;graph search;data clustering;data classification;Map Reduce framework;Hadoop distributed file system;minimization technique;file indexing;mapping;sorting;shuffling;Big data;Program processors;Computer architecture;File systems;Fault tolerance;Fault tolerant systems;Distributed databases},
doi={10.1109/ICITCS.2014.7021746},
ISSN={},
month={Oct},}

@ARTICLE{dsmmng,
author={B. {Nitzberg} and V. {Lo}},
journal={Computer}, 
title={Distributed shared memory: a survey of issues and algorithms}, 
year={1991},
volume={24},
number={8},
pages={52-60},
abstract={An overview of distributed shared memory (DSM) issues is presented. Memory coherence, design choices, and implementation methods are included. The discussion of design choices covers structure and granularity, coherence semantics, scalability, and heterogeneity. Implementation issues concern data location and access, the coherence protocol, replacement strategy, and thrashing. Algorithms that support process synchronization and memory management are discussed.<>},
keywords={data handling;distributed processing;storage management;memory coherence;memory design choices;data access;distributed shared memory;DSM;granularity;coherence semantics;scalability;heterogeneity;data location;coherence protocol;replacement strategy;thrashing;process synchronization;memory management;Data handling;Distributed computing;Memory management},
doi={10.1109/2.84877},
ISSN={1558-0814},
month={Aug},}

